SQL优化器核心执行策略主要分为两个大的方向：基于规则优化（CRO）以及基于代价优化(CBO)

# CRO(基于规则优化)

基于规则优化是一种经验式、启发式地优化思路，更多地依靠前辈总结出来的优化规则，简单易行且能够覆盖到大部分优化逻辑。

> 但是对于核心优化算子Join却显得有点力不从心。举个简单的例子:两个表执行Join到底应该使用BroadcastJoin ? ShuffleHashJoin ? SortMergeJoin

![图片alt](./img/o1.png)

## 预备知识－Tree&Rule

在介绍SQL优化器工作原理之前，有必要首先介绍两个重要的数据结构：Tree和Rule。相信无论对SQL优化器有无了解，都肯定知道SQL语法树这个概念，不错，SQL语法树就是SQL语句通过编译器之后会被解析成一棵树状结构。这棵树会包含很多节点对象，每个节点都拥有特定的数据类型，同时会有0个或多个孩子节点（节点对象在代码中定义为TreeNode对象），下图是个简单的示例

![图片alt](./img/o2.png)

如上图所示，箭头左边表达式有3种数据类型（Literal表示常量、Attribute表示变量、Add表示动作），表示x+(1+2)。映射到右边树状结构后，每一种数据类型就会变成一个节点。另外，Tree还有一个非常重要的特性，可以通过一定的规则进行等价变换，如下图

![图片alt](./img/o3.png)

上图定义了一个等价变换规则(Rule)：两个Integer类型的常量相加可以等价转换为一个Integer常量，这个规则其实很简单，对于上文中提到的表达式x+(1+2)来说就可以转变为x+3。对于程序来讲，如何找到两个Integer常量呢？其实就是简单的二叉树遍历算法，每遍历到一个节点，就模式匹配当前节点为Add、左右子节点是Integer常量的结构，定位到之后将此三个节点替换为一个Literal类型的节点。
上面用一个最简单的示例来说明等价变换规则以及如何将规则应用于语法树。在任何一个SQL优化器中，通常会定义大量的Rule（后面会讲到），SQL优化器会遍历语法树中每个节点，针对遍历到的节点模式匹配所有给定规则（Rule），如果有匹配成功的，就进行相应转换，如果所有规则都匹配失败，就继续遍历下一个节点

## Catalyst工作流程

任何一个优化器工作原理都大同小异：SQL语句首先通过Parser模块被解析为语法树，此棵树称为Unresolved Logical Plan；Unresolved Logical Plan通过Analyzer模块借助于数据元数据解析为Logical Plan；此时再通过各种基于规则的优化策略进行深入优化，得到Optimized Logical Plan；优化后的逻辑执行计划依然是逻辑的，并不能被Spark系统理解，此时需要将此逻辑执行计划转换为Physical Plan；为了更好的对整个过程进行理解，下文通过一个简单示例进行解释

### Parser

Parser简单来说是将SQL字符串切分成一个一个Token，再根据一定语义规则解析为一棵语法树（AST）。Parser模块目前基本都使用第三方类库ANTLR进行实现，比如Hive、 Presto、SparkSQL等。下图是一个示例性的SQL语句（有两张表，其中people表主要存储用户基本信息，score表存储用户的各种成绩），通过Parser解析后的AST语法树如右图所示

![图片alt](./img/o4.png)

### Analyzer

通过解析后的逻辑执行计划基本有了骨架，但是系统并不知道score、sum这些都是些什么鬼，此时需要基本的元数据信息来表达这些词素，最重要的元数据信息主要包括两部分：表的Scheme和基本函数信息，表的scheme主要包括表的基本定义（列名、数据类型）、表的数据格式（Json、Text）、表的物理位置等，基本函数信息主要指类信息。
Analyzer会再次遍历整个语法树，对树上的每个节点进行数据类型绑定以及函数绑定，比如people词素会根据元数据表信息解析为包含age、id以及name三列的表，people.age会被解析为数据类型为int的变量，sum会被解析为特定的聚合函数，如下图所示

![图片alt](./img/o5.png)

SparkSQL中Analyzer定义了各种解析规则，有兴趣深入了解的童鞋可以查看Analyzer类，其中定义了基本的解析规则，如下

![图片alt](./img/o6.png)

### Optimizer

优化器是整个Catalyst的核心，上文提到优化器分为基于规则优化和基于代价优化两种，当前SparkSQL 2.1依然没有很好的支持基于代价优化（下文细讲），此处只介绍基于规则的优化策略，基于规则的优化策略实际上就是对语法树进行一次遍历，模式匹配能够满足特定规则的节点，再进行相应的等价转换。因此，基于规则优化说到底就是一棵树等价地转换为另一棵树。SQL中经典的优化规则有很多，下文结合示例介绍三种比较常见的规则：谓词下推（Predicate Pushdown）、常量累加（Constant Folding）和列值裁剪（Column Pruning）

#### Predicate Pushdown

![图片alt](./img/o7.png)

上图左边是经过Analyzer解析后的语法树，语法树中两个表先做join，之后再使用age>10对结果进行过滤。大家知道join算子通常是一个非常耗时的算子，耗时多少一般取决于参与join的两个表的大小，如果能够减少参与join两表的大小，就可以大大降低join算子所需时间。谓词下推就是这样一种功能，它会将过滤操作下推到join之前进行，上图中过滤条件age>0以及id!=null两个条件就分别下推到了join之前。这样，系统在扫描数据的时候就对数据进行了过滤，参与join的数据量将会得到显著的减少，join耗时必然也会降低

#### Constant Folding

![图片alt](./img/o8.png)

常量累加其实很简单，就是上文中提到的规则  x+(1+2)  -> x+3，虽然是一个很小的改动，但是意义巨大。示例如果没有进行优化的话，每一条结果都需要执行一次100+80的操作，然后再与变量math_score以及english_score相加，而优化后就不需要再执行100+80操作

#### Column Pruning

![图片alt](./img/o9.png)

列值裁剪是另一个经典的规则，示例中对于people表来说，并不需要扫描它的所有列值，而只需要列值id，所以在扫描people之后需要将其他列进行裁剪，只留下列id。这个优化一方面大幅度减少了网络、内存数据量消耗，另一方面对于列存数据库（Parquet）来说大大提高了扫描效率。除此之外，Catalyst还定义了很多其他优化规则，有兴趣深入了解的童鞋可以查看Optimizer类，下图简单的截取一部分规则

![图片alt](./img/o10.png)

至此，逻辑执行计划已经得到了比较完善的优化，然而，逻辑执行计划依然没办法真正执行，他们只是逻辑上可行，实际上Spark并不知道如何去执行这个东西。比如Join只是一个抽象概念，代表两个表根据相同的id进行合并，然而具体怎么实现这个合并，逻辑执行计划并没有说明

### 逻辑执行计划转换为物理执行计划

此时就需要将逻辑执行计划转换为物理执行计划，将逻辑上可行的执行计划变为Spark可以真正执行的计划。比如Join算子，Spark根据不同场景为该算子制定了不同的算法策略，有BroadcastHashJoin、ShuffleHashJoin以及SortMergeJoin等（可以将Join理解为一个接口，BroadcastHashJoin是其中一个具体实现），物理执行计划实际上就是在这些具体实现中挑选一个耗时最小的算法实现，这个过程涉及到基于代价优化策略，后续文章细讲

![图片alt](./img/o11.png)

# CBO(基于代价优化)

CBO是Cost-Based Optimization的缩写。它是看语句的代价(Cost),这里的代价主要指Cpu和内存。优化器在判断是否用这种方式时,主要参照的是表及索引的统计信息。统计信息给出表的大小、多少行、每行的长度等信息。这些统计信息起初在库内是没有的，是做analyze后才出现的，很多的时候过期统计信息会令优化器做出一个错误的执行计划,因些应及时更新这些信息.

CRO是一种经验式、启发式的优化思路，优化规则都已经预先定义好，只需要将SQL往这些规则上套就可以。 说白了，CRO就像是一个经验丰富的老司机，基本套路全都知道。然而世界上有一种东西叫做 - 不按套路来，与其说它不按套路来，倒不如说它本身就没有什么套路而言。最典型的莫过于复杂Join算子，对于这些复杂的Join来说，通常有两个对优化相当重要的问题需要决定：

  1. 该Join应该选择哪种策略来执行？BroadcastJoin or ShuffleHashJoin or SortMergeJoin？不同的执行策略对系统的资源要求不同，执行效率也有天壤之别，同一个SQL，选择到合适的策略执行可能只需要几秒钟，而如果没有选择到合适的执行策略就可能会导致系统OOM

  2. 对于雪花模型或者星型模型来讲，多表Join应该选择什么样的顺序执行？不同的Join顺序意味着不同的执行效率，比如A join B join C，A、B表都很大，C表很小，那A join B很显然需要大量的系统资源来运算，执行时间肯定不会短。而如果使用A join C join B的执行顺序，因为C表很小，所以A join C会很快得到结果，而且结果集会很小，再使用小的结果集 join B，结果必然也会很小

首先来看第一个问题，当前SparkSQL会让用户指定参数'spark.sql.autoBroadcastJoinThreshold’来决定是否采用BroadcastJoin策略，简单来说，它会选择参与Join的两表中的小表大小与该值进行对比，如果小表大小小于该配置值，就将此表进行广播；否则采用SortMergeJoin策略。对于SparkSQL采取的方式，有两个问题需要深入分析
  - 参数'spark.sql.autoBroadcastJoinThreshold’指定的是表的大小（size），而不是条数。这样完全合理吗？我们知道Join算子会将两表中具有相同key的记录合并在一起，因此Join的复杂度只与两表的数据条数有关，而与表大小（size）没有直接的关系。这样一想，其实参数'spark.sql.autoBroadcastJoinThreshold’应该更多地是考虑广播的代价，而不是Join本身的代价

  - 之前Catalyst文章中我们讲到谓词下推规则，Catalyst会将很多过滤条件下推到Join之前，因此参与Join的两表大小并不应该是两张原始表的大小，而是经过过滤后的表数据大小。因此，单纯的知道原始表大小还远远不够，Join优化还需要评估过滤后表数据大小以及表数据条数

对于第二个问题，也面临和第一个问题同样的两点缺陷，举个简单的例子

```sql
select * from A , B , C where A.id = B.b_id and A.id = C.c_id and C.c_id > 100
```

> 假设：A、B、C总纪录大小分别是100m，40m，60m，C.c_id > 100过滤后C的总纪录数会降到10m

上述SQL是一个典型的A join B join C的多Join示例，很显然，A肯定在最前面，现在问题是B和C的Join顺序，是A join B join C还是A join C join B。对于上面的示例，优化器会有两种最基本的选择，第一就是按照用户手写的Join顺序执行，即按照‘A.id = B.b_id and A.id = C.c_id ’顺序， 得到的执行顺序是A join B join C。第二是按照A、B 、C三表的原始大小进行组织排序，原始表小的先Join，原始表大的后Join，适用这种规则得到的顺序依然是A join B join C，因为B的记录大小小于C的记录大小

同样的道理，第一个缺陷很明显，记录大小并不能精确作为Join代价的计算依据，而应该是记录条数。第二就是对于过滤条件的忽略，上述示例中C经过过滤后的大小降到10m，明显小于B的40m，因此实际上应该执行的顺序为A join C join B，与上述得到的结果刚好相反

可见，基于规则的优化策略并不适合复杂Join的优化，此时就需要另一种优化策略 - 基于代价优化（CBO）。基于代价优化策略实际只做两件事，就是解决上文中提到的两个问题

  1. 解决参与Join的数据表大小并不能完全作为计算Join代价依据的问题，而应该加入数据记录条数这个维度

  2. 解决Join代价计算应该考虑谓词下推（等）条件的影响，不能仅仅关注原始表的大小。这个问题看似简单，实际上很复杂，需要优化器将逻辑执行计划树上的每一个节点的<数据量，数据条数>都评估出来，这样的话，评估Join的执行策略、执行顺序就不再以原始表大小作为依据，而是真实参与Join的两个数据集的大小条数作为依据

## CBO实现思路

经过上文的解释，可以明确CBO的本质就是计算LogionPlan每个节点的输出数据大小与数据条数，作为复杂Join算子的代价计算依据。逻辑执行计划树的叶子节点是原始数据，往上会经过各种过滤条件以及其他函数的限制，父节点依赖于子节点。整个过程可以变换为子节点经过特定计算评估父节点的输出，计算出来之后父节点将会作为上一层的子节点往上计算。因此，CBO可以分解为两步

  1. 一次性计算出原始数据的相关数据
  2. 再对每类节点制定一种对应的评估规则就可以自下往上评估出所有节点的代价值

### 一次性计算出原始表的相关数据

这个操作是CBO最基础的一项工作，在计算之前，我们需要明确“相关数据”是什么？这里给出核心的统计信息如下
  - estimatedSize: 每个LogicalPlan节点输出数据大小（解压）
  - rowCount: 每个LogicalPlan节点输出数据总条数
  - basicStats: 基本列信息，包括列类型、Max、Min、number of nulls, number of distinct values, max column length, average column length等
  - Histograms: Histograms of columns, i.e., equi-width histogram (for numeric and string types) and equi-height histogram (only for numeric types).

至于为什么要统计这么多数据，下文会讲到。现在再来看如何进行统计，有两种比较可行的方案
  1. 打开所有表扫描一遍，这样最简单，而且统计信息准确，缺点是对于大表来说代价比较大。hive和impala目前都采用的这种方式
    - hive统计原始表命令：analyse table
    - impala统计原始表明了：compute stats
  2. 针对一些大表，扫描一遍代价太大，可以采用采样（sample）的方式统计计算

### 代价评估规则&计算所有节点统计信息

代价评估规则意思是说在当前子节点统计信息的基础上，计算父节点相关统计信息的一套规则。 对于不同谓词节点，评估规则必然不一样，比如fliter、group by、limit等等的评估规则不同。 假如现在已经知道表C的基本统计信息，对于

```sql
select * from A , B , C where A.id = B.b_id and A.id = C.c_id and C.c_id > N
```

这个条件，如何评估经过C.c_id > N过滤后的基本统计信息。我们来看看

  1. 假设当前已知C列的最小值c_id.Min、最大值c_id.Max以及总行数c_id.Distinct，如下图所示

  ![图片alt](./img/o12.png)

  2. 现在分别有三种情况需要说明，其一是N小于c_id.Min，其二是N大于c_id.Max，其三是N介于c_id.Min和c_id.Max之间。前两种场景是第三种场景的特殊情况，这里简单的针对第三种场景说明。如下图所示

  ![图片alt](./img/o13.png)

在C.c_id > N过滤条件下，c_id.Min会增大到N，c_id.Max保持不变。而过滤后总行数

```
c_id.distinct(after filter) ＝ (c_id.Max - N) / (c_id.Max - c_id.Min) * c_id.distinct(before filter)
```

当然，上述计算只是示意性计算，真实算法会复杂很多。另外，如果大家对group by 、limit等谓词的评估规则比较感兴趣的话，可以阅读 [SparkSQL CBO设计文档](./Spark_CBO_Design_Spec.pdf)  ，在此不再赘述。至此，通过各种评估规则就可以计算出语法树中所有节点的基本统计信息，当然最重要的是参与Join的数据集节点的统计信息。最后只需要根据这些统计信息选择最优的Join算法以及Join顺序，最终得到最优的物理执行计划
